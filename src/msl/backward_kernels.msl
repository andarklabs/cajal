#include <metal_stdlib>
using namespace metal;

// Backward pass for Layer Normalization
// Given grad_output (dL/dy_norm), input_plus_residual (x from forward pass),
// gamma, and normalized_x_component = (x - mean) / sqrt(variance + eps)
// Computes: dL/dgamma, dL/dbeta, dL/dx (gradient of loss w.r.t. input_plus_residual)
kernel void layer_norm_backward(
    device const half* grad_output_norm [[buffer(0)]],      // dL/dy_L (B, S, E)
    device const half* input_plus_residual [[buffer(1)]],   // x = input_sublayer + residual (B, S, E) (saved from forward)
    device const half* gamma [[buffer(2)]],                 // gamma weights (E)
    device const float* mean_per_instance [[buffer(3)]],      // mean of x for each instance (B, S)
    device const float* rsqrt_variance_per_instance [[buffer(4)]], // 1/sqrt(variance + eps) for each instance (B, S)
    
    device atomic_float* grad_gamma [[buffer(5)]],              // dL/dgamma (E) (atomic accumulation)
    device atomic_float* grad_beta [[buffer(6)]],               // dL/dbeta (E) (atomic accumulation)
    device half* grad_input_plus_residual [[buffer(7)]],    // dL/dx (B, S, E)
    
    constant uint& batch_size [[buffer(8)]],
    constant uint& seq_len [[buffer(9)]],
    constant uint& embedding_dim [[buffer(10)]],
    uint2 threadgroup_id [[threadgroup_position_in_grid]], // Threadgroup position in 2D grid
    uint2 thread_id [[thread_position_in_threadgroup]]     // Thread position within threadgroup
) {
    uint instance_idx = threadgroup_id.x;  // Which instance (batch*seq position)
    uint dim_idx = thread_id.y;            // Which embedding dimension (0 to embedding_dim-1)
    if (instance_idx >= batch_size * seq_len || dim_idx >= embedding_dim) return;

    // Threadgroup arrays for manual reduction.
    // Ensure embedding_dim passed to this kernel is <= 1024 when launching,
    // or use dynamic threadgroup memory allocation if supported and needed.
    threadgroup float tg_sum_dL_dnorm_x[1024]; 
    threadgroup float tg_sum_dL_dnorm_x_mul_x_centered[1024];

    // Bounds check for threadgroup arrays (defensive, should be handled by launch config)
    if (embedding_dim > 1024) {
        // This indicates a programming error or misconfiguration from the host.
        // Kernels should ideally be robust or configured not to hit this.
        return; 
    }

    uint base_offset = instance_idx * embedding_dim;
    float x_i = float(input_plus_residual[base_offset + dim_idx]);
    float grad_output_norm_i = float(grad_output_norm[base_offset + dim_idx]);
    float gamma_i = float(gamma[dim_idx]); // gamma is (E), so indexed by dim_idx
    float mean_i = mean_per_instance[instance_idx];
    float rsqrt_var_i = rsqrt_variance_per_instance[instance_idx];

    // Accumulate dL/dbeta and dL/dgamma
    atomic_fetch_add_explicit(&grad_beta[dim_idx], grad_output_norm_i, memory_order_relaxed);
    float x_centered_norm_i = (x_i - mean_i) * rsqrt_var_i;
    atomic_fetch_add_explicit(&grad_gamma[dim_idx], grad_output_norm_i * x_centered_norm_i, memory_order_relaxed);

    float dL_dnorm_x_i = grad_output_norm_i * gamma_i;

    // Store partial sums in threadgroup memory
    // Initialize only elements this thread is responsible for if embedding_dim < threads_per_threadgroup
    tg_sum_dL_dnorm_x[dim_idx] = dL_dnorm_x_i;
    tg_sum_dL_dnorm_x_mul_x_centered[dim_idx] = dL_dnorm_x_i * (x_i - mean_i); // x_centered, not x_centered_norm
    
    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Manual reduction in threadgroup memory.
    // This reduction works correctly if embedding_dim is the number of threads used for reduction
    // (i.e., threads_per_threadgroup in this dimension == embedding_dim).
    // It assumes embedding_dim is <= 1024.
    for (uint s = embedding_dim / 2; s > 0; s >>= 1) {
        if (dim_idx < s) {
            // Check to prevent reading beyond actual 'embedding_dim' if it's not a power of 2
            // and less than the allocated threadgroup array size.
            // However, if threads are launched for 'embedding_dim', this check might not be needed
            // as 'dim_idx + s' would be valid thread indices writing to those locations.
            // The key is that only 'dim_idx < s' threads do the writing.
            tg_sum_dL_dnorm_x[dim_idx] += tg_sum_dL_dnorm_x[dim_idx + s];
            tg_sum_dL_dnorm_x_mul_x_centered[dim_idx] += tg_sum_dL_dnorm_x_mul_x_centered[dim_idx + s];
        }
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }

    // Only thread 0 in the group uses the final sums
    if (dim_idx == 0) {
        float final_sum_dL_dnorm_x = tg_sum_dL_dnorm_x[0];
        float final_sum_dL_dnorm_x_mul_x_centered = tg_sum_dL_dnorm_x_mul_x_centered[0];

        // This part of calculation is now done by thread 0, but dL_dx_i needs to be calculated by each thread.
        // The reduction results are needed by ALL threads in the threadgroup to compute their dL_dx_i.
        // So, the reduced sums should be read by all threads AFTER the reduction is complete.
        // The barrier above ensures this. The following calculation is per-thread (dim_idx).
    }
    // All threads need the reduction results. The barrier ensures tg_sum...[0] are ready.
    float final_sum_dL_dnorm_x = tg_sum_dL_dnorm_x[0]; // All threads read this
    float final_sum_dL_dnorm_x_mul_x_centered = tg_sum_dL_dnorm_x_mul_x_centered[0]; // All threads read this

    float term1_factor = 1.0f / float(embedding_dim);
    float term_sum_dL_dnorm_x_mul_x_centered = final_sum_dL_dnorm_x_mul_x_centered * (x_i - mean_i) * rsqrt_var_i * rsqrt_var_i;
    float term_sum_dL_dnorm_x = final_sum_dL_dnorm_x;

    float dL_dx_i = rsqrt_var_i * (
                        dL_dnorm_x_i - 
                        (term_sum_dL_dnorm_x_mul_x_centered + term_sum_dL_dnorm_x) * term1_factor
                    );

    grad_input_plus_residual[base_offset + dim_idx] = half(dL_dx_i);
}

// Backward pass for Output Logits Projection
// Y = XW + b  =>  dL/dX = dL/dY @ W^T, dL/dW = X^T @ dL/dY, dL/db = sum(dL/dY)
kernel void output_projection_backward(
    device const float* grad_logits [[buffer(0)]],          // dL/dY (B, S, V)
    device const half* final_hidden_states [[buffer(1)]],   // X (B, S, E) (input to this layer from forward)
    device const half* W_out [[buffer(2)]],                 // W (E, V)
    
    device atomic_float* grad_W_out [[buffer(3)]],              // dL/dW (E, V) (atomic accumulation)
    device atomic_float* grad_b_out [[buffer(4)]],              // dL/db (V) (atomic accumulation)
    device half* grad_final_hidden_states [[buffer(5)]],    // dL/dX (B, S, E)
    
    constant uint& batch_size [[buffer(6)]],
    constant uint& seq_len [[buffer(7)]],
    constant uint& embedding_dim [[buffer(8)]],
    constant uint& vocab_size [[buffer(9)]],
    // It's generally better to dispatch 2D or 3D grids where each dimension maps to a logical loop.
    // E.g., grid (instance_count, embedding_dim_count, vocab_count)
    // Current gid: gid.x for instance_idx, gid.y for e_idx, gid.z for v_idx
    uint3 gid [[thread_position_in_grid]] 
) {
    uint instance_idx = gid.x; // instance = batch_idx * sequence_length + seq_idx
    uint e_idx = gid.y;        // for dL/dX (row of W_out in W_out^T) and dL/dW (row of W_out)
    uint v_idx = gid.z;        // for dL/db and dL/dW (col of W_out)

    // Calculate dL/db_out[v_idx] = sum_instances(grad_logits[instance_idx, v_idx])
    // Each thread (instance_idx, 0, v_idx) contributes its grad_logits[instance_idx, v_idx] to grad_b_out[v_idx]
    if (e_idx == 0 && instance_idx < batch_size * seq_len && v_idx < vocab_size) {
        atomic_fetch_add_explicit(&grad_b_out[v_idx], grad_logits[instance_idx * vocab_size + v_idx], memory_order_relaxed);
    }

    // Calculate dL/dW_out[e_idx, v_idx] = sum_instances( final_hidden_states[instance_idx, e_idx] * grad_logits[instance_idx, v_idx] )
    // Each thread (instance_idx, e_idx, v_idx) contributes one term to the sum.
    if (instance_idx < batch_size * seq_len && e_idx < embedding_dim && v_idx < vocab_size) {
        float x_ie = float(final_hidden_states[instance_idx * embedding_dim + e_idx]);
        float dY_iv = grad_logits[instance_idx * vocab_size + v_idx];
        atomic_fetch_add_explicit(&grad_W_out[e_idx * vocab_size + v_idx], x_ie * dY_iv, memory_order_relaxed);
    }
    
    // Calculate grad_final_hidden_states[instance_idx, e_idx] (dL/dX)
    // dL/dX[i,e] = sum_v ( dL/dY[i,v] * W_out[e,v] )
    // Each thread (instance_idx, e_idx, 0) computes one element of dL/dX by summing over v_idx (k in original code).
    // This loop over vocab_size can be very long if vocab_size is large, potentially causing TDR.
    // Consider parallelizing this sum if vocab_size is large (e.g., tiled reduction or multiple kernels).
    if (v_idx == 0 && instance_idx < batch_size * seq_len && e_idx < embedding_dim) { 
        float sum_val = 0.0f;
        for (uint k = 0; k < vocab_size; ++k) { // k here is the vocab_size dimension
            // Ensure k is within bounds for W_out access if W_out's second dim isn't vocab_size
            // (though W_out is E,V, so W_out[e_idx * vocab_size + k] is fine)
            float dY_ik = grad_logits[instance_idx * vocab_size + k]; 
            float W_ek = float(W_out[e_idx * vocab_size + k]); 
            sum_val += dY_ik * W_ek;
        }
        grad_final_hidden_states[instance_idx * embedding_dim + e_idx] = half(sum_val);
    }
}

// GELU derivative helper function
float gelu_derivative(float x) {
    const float sqrt_2_over_pi = 0.7978845608f; // sqrt(2/M_PI_F)
    const float a = 0.044715f;
    
    float x_cubed = x * x * x;
    float inner_term = sqrt_2_over_pi * (x + a * x_cubed);
    float tanh_inner = tanh(inner_term);
    
    // derivative of inner_term w.r.t x
    float d_inner_dx = sqrt_2_over_pi * (1.0f + 3.0f * a * x * x);
    
    // sech^2(y) = 1 - tanh^2(y)
    float sech_sq_inner = 1.0f - tanh_inner * tanh_inner;
    
    float derivative = 0.5f * (1.0f + tanh_inner) + 0.5f * x * sech_sq_inner * d_inner_dx;
    return derivative;
}

// Feed-Forward Network Backward Pass Kernel (Rewritten for better parallelism)
// FFN Forward: X -> H_lin = X@W1 + b1 -> H_act = GELU(H_lin) -> Y = H_act@W2 + b2
// FFN Backward: dL/dY -> compute all gradients and dL/dX
kernel void ffn_backward(
    device const half* grad_ffn_output,     // [B, S, E] dL/dY
    device const half* ffn_input,           // [B, S, E] X (saved from forward)
    device const half* h_linear,            // [B, S, H] H_lin (saved from forward)
    device const half* h_activated,         // [B, S, H] H_act (saved from forward)
    device const half* w1,                  // [E, H] first layer weights
    device const half* w2,                  // [H, E] second layer weights

    device atomic_float* grad_w1,           // [E, H] gradient output (atomic)
    device atomic_float* grad_b1,           // [H] gradient output (atomic)
    device atomic_float* grad_w2,           // [H, E] gradient output (atomic)
    device atomic_float* grad_b2,           // [E] gradient output (atomic)
    device half* grad_ffn_input,            // [B, S, E] gradient output (dL/dX)

    // Temporary buffers needed for intermediate gradients if computed in stages by same kernel
    // Or use separate kernels. For one kernel, these might be threadgroup or device.
    // Assuming ffn_hidden_dim and embedding_dim fit into threadgroup for parts of computation.
    // Let's try to compute dL/dH_lin and dL/dH_act on the fly or using threadgroup memory per instance.
    
    constant uint& batch_size,
    constant uint& seq_len,
    constant uint& embedding_dim,       // E
    constant uint& ffn_hidden_dim,      // H

    // gid.x = instance_idx (0 to B*S - 1)
    // gid.y = h_idx (0 to ffn_hidden_dim - 1) for some calcs, e_idx for others
    // gid.z = e_idx (0 to embedding_dim - 1) for some calcs
    // This kernel is complex. A common strategy:
    // Thread (instance_idx, h_idx, e_idx)
    // This dispatch is (B*S, H, E)
    uint3 gid [[thread_position_in_grid]]
) {
    uint instance_idx = gid.x;
    uint h_idx = gid.y; // Corresponds to a hidden dimension index
    uint e_idx = gid.z; // Corresponds to an embedding dimension index

    if (instance_idx >= batch_size * seq_len) return;

    uint input_base = instance_idx * embedding_dim;
    uint hidden_base = instance_idx * ffn_hidden_dim;

    // --- Step 1: Compute dL/dW2, dL/db2, and dL/dH_act ---
    // Each thread (instance_idx, h_idx, e_idx) processes one (h,e) element for dL/dW2
    // and contributes to dL/dH_act and dL/db2.

    // dL/dW2[h,e] += H_act[instance, h] * grad_ffn_output[instance, e]
    if (h_idx < ffn_hidden_dim && e_idx < embedding_dim) {
        float h_act_val = float(h_activated[hidden_base + h_idx]);
        float grad_out_val = float(grad_ffn_output[input_base + e_idx]);
        atomic_fetch_add_explicit(&grad_w2[h_idx * embedding_dim + e_idx], h_act_val * grad_out_val, memory_order_relaxed);
    }

    // dL/db2[e] += grad_ffn_output[instance, e]
    // Only threads with h_idx = 0 perform this to avoid multiple additions for the same (instance,e)
    if (h_idx == 0 && e_idx < embedding_dim) {
        float grad_out_val = float(grad_ffn_output[input_base + e_idx]);
        atomic_fetch_add_explicit(&grad_b2[e_idx], grad_out_val, memory_order_relaxed);
    }

    // 🔧 INTERIM SOLUTION: Fix threadgroup reductions with proper initialization
    // This fixes the immediate NaN bug while we design the full atomic solution
    
    // 🔧 OPTIMIZED: Each thread initializes only its own elements
    threadgroup float tg_sum_for_dLdHact[1024];
    if (e_idx < 1024) { // Only threads within bounds initialize
        tg_sum_for_dLdHact[e_idx] = 0.0f;
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);
    
    // Now the original reduction algorithm will work correctly
    if (embedding_dim > 1024) return; // Safety check
    
    if (h_idx < ffn_hidden_dim && e_idx < embedding_dim) {
        float grad_out_val = float(grad_ffn_output[input_base + e_idx]);
        float w2_val = float(w2[h_idx * embedding_dim + e_idx]);
        tg_sum_for_dLdHact[e_idx] = grad_out_val * w2_val;
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);

    float dL_dH_act_h = 0.0f;
    if (h_idx < ffn_hidden_dim) {
        // Safe reduction with zero-initialized memory
        for (uint s = embedding_dim / 2; s > 0; s >>= 1) {
            if (e_idx < s && (e_idx + s) < 1024) { // Bounds check
                tg_sum_for_dLdHact[e_idx] += tg_sum_for_dLdHact[e_idx + s];
            }
            threadgroup_barrier(mem_flags::mem_threadgroup);
        }
        if (e_idx == 0) {
            dL_dH_act_h = tg_sum_for_dLdHact[0];
        }
    }
    
    // Simple broadcast using threadgroup memory
    threadgroup float tg_dL_dH_act_h_val;
    if (e_idx == 0 && h_idx < ffn_hidden_dim) {
        tg_dL_dH_act_h_val = dL_dH_act_h;
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);
    if (h_idx < ffn_hidden_dim) {
        dL_dH_act_h = tg_dL_dH_act_h_val;
    }


    // --- Step 2: Compute dL/dH_lin ---
    // dL/dH_lin[instance, h] = dL/dH_act[instance, h] * GELU'(H_lin[instance,h])
    float dL_dH_lin_h = 0.0f;
    if (h_idx < ffn_hidden_dim && e_idx == 0) { // Let thread e_idx=0 compute this
        float h_lin_val = float(h_linear[hidden_base + h_idx]);
        float gelu_deriv_val = gelu_derivative(h_lin_val);
        dL_dH_lin_h = dL_dH_act_h * gelu_deriv_val;
    }
    
    // Broadcast dL_dH_lin_h to other threads
    threadgroup float tg_dL_dH_lin_h_val;
    if (e_idx == 0 && h_idx < ffn_hidden_dim) {
        tg_dL_dH_lin_h_val = dL_dH_lin_h;
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);
    if (h_idx < ffn_hidden_dim) {
        dL_dH_lin_h = tg_dL_dH_lin_h_val;
    }


    // --- Step 3: Compute dL/dW1, dL/db1, and dL/dX (grad_ffn_input) ---
    // dL/dW1[e,h] += ffn_input[instance,e] * dL/dH_lin[instance,h]
    if (e_idx < embedding_dim && h_idx < ffn_hidden_dim) {
        float x_val = float(ffn_input[input_base + e_idx]);
        // dL_dH_lin_h is for (instance_idx, h_idx), correctly obtained above
        atomic_fetch_add_explicit(&grad_w1[e_idx * ffn_hidden_dim + h_idx], x_val * dL_dH_lin_h, memory_order_relaxed);
    }

    // dL/db1[h] += dL/dH_lin[instance,h]
    // Only threads with e_idx = 0 perform this
    if (e_idx == 0 && h_idx < ffn_hidden_dim) {
        // dL_dH_lin_h is for (instance_idx, h_idx)
        atomic_fetch_add_explicit(&grad_b1[h_idx], dL_dH_lin_h, memory_order_relaxed);
    }

    // dL/dX[instance,e] = sum_h (dL/dH_lin[instance,h] * w1[e,h])
    // 🔧 FIXED: Zero-initialize threadgroup array for safe reduction
    threadgroup float tg_sum_for_dLdX[2048]; // Max H, ensure ffn_hidden_dim <= 2048
    
    // 🔧 OPTIMIZED: Each thread initializes only its own elements  
    if (h_idx < 2048) { // Only threads within bounds initialize
        tg_sum_for_dLdX[h_idx] = 0.0f;
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);

    if (ffn_hidden_dim > 2048) return; // Safety check

    if (e_idx < embedding_dim && h_idx < ffn_hidden_dim) {
        float w1_val = float(w1[e_idx * ffn_hidden_dim + h_idx]);
        tg_sum_for_dLdX[h_idx] = dL_dH_lin_h * w1_val;
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);
    
    float dL_dX_e = 0.0f;
    if (e_idx < embedding_dim) {
        // Safe reduction with bounds checking
        for (uint s = ffn_hidden_dim / 2; s > 0; s >>= 1) {
            if (h_idx < s && (h_idx + s) < 2048) { // Bounds check
                tg_sum_for_dLdX[h_idx] += tg_sum_for_dLdX[h_idx + s];
            }
            threadgroup_barrier(mem_flags::mem_threadgroup);
        }
        if (h_idx == 0) { // Thread h_idx=0 holds the final sum
            dL_dX_e = tg_sum_for_dLdX[0];
            grad_ffn_input[input_base + e_idx] = half(dL_dX_e);
        }
    }
}


// Backward pass for MHSA Output Projection
kernel void mhsa_output_projection_backward(
    device const half* grad_mhsa_output [[buffer(0)]],           // dL/dY (B, S, E)
    device const half* concatenated_attention_heads [[buffer(1)]], // X (B, S, E) (saved from forward)
    device const half* W_o [[buffer(2)]],                         // W_o (E, E)
    
    device atomic_float* grad_W_o [[buffer(3)]],                  // dL/dW_o (E, E) (atomic accumulation)
    device atomic_float* grad_b_o [[buffer(4)]],                  // dL/db_o (E) (atomic accumulation)
    device half* grad_concatenated_attention_heads [[buffer(5)]], // dL/dX (B, S, E)
    
    constant uint& batch_size [[buffer(6)]],
    constant uint& seq_len [[buffer(7)]],
    constant uint& embedding_dim [[buffer(8)]],
    // gid.x for instance_idx, gid.y for e_out_idx, gid.z for e_in_idx (can be mapped to e_out_idx for dL/dX)
    uint3 gid [[thread_position_in_grid]] 
) {
    uint instance_idx = gid.x; 
    uint e_out_idx    = gid.y; // Corresponds to an output dimension of X or row of W_o in W_o^T
    uint e_in_idx     = gid.z; // Corresponds to an input dimension of X or col of W_o / row of W_o

    // Calculate dL/db_o[e_out_idx]
    // Each thread (instance_idx, e_out_idx, 0) contributes.
    if (e_in_idx == 0 && instance_idx < batch_size * seq_len && e_out_idx < embedding_dim) {
        float grad_y_ie = float(grad_mhsa_output[instance_idx * embedding_dim + e_out_idx]);
        atomic_fetch_add_explicit(&grad_b_o[e_out_idx], grad_y_ie, memory_order_relaxed);
    }

    // Calculate dL/dW_o[e_in_idx, e_out_idx] (W_o is E_in x E_out, so grad_W_o is E_in x E_out)
    // Standard is dL/dW[in, out] += X[instance, in] * dL/dY[instance, out]
    // Here, W_o is (E, E). Let's say W_o[row, col] means W_o[input_feature_to_linear, output_feature_from_linear]
    // If X (concat_heads) is (B*S, E_in) and W_o is (E_in, E_out), Y is (B*S, E_out)
    // Here X is (B*S, E) and W_o is (E,E). So E_in = E, E_out = E.
    // grad_W_o[e_row_of_W, e_col_of_W]
    // Using e_in_idx for row of W_o, e_out_idx for col of W_o
    if (instance_idx < batch_size * seq_len && e_in_idx < embedding_dim && e_out_idx < embedding_dim) {
        float x_val = float(concatenated_attention_heads[instance_idx * embedding_dim + e_in_idx]); // X[instance, e_in_idx]
        float dY_val = float(grad_mhsa_output[instance_idx * embedding_dim + e_out_idx]);      // dY[instance, e_out_idx]
        atomic_fetch_add_explicit(&grad_W_o[e_in_idx * embedding_dim + e_out_idx], x_val * dY_val, memory_order_relaxed);
    }
    
    // Calculate dL/dX[instance_idx, e_out_idx] (grad_concatenated_attention_heads)
    // dL/dX[i,j] = sum_k ( dL/dY[i,k] * W_o[j,k] ) 
    // (Mapping: i=instance_idx, j=e_out_idx (row of X), k=summation index over output features of Y / cols of W_o)
    // Let W_o be indexed W_o[row_idx_from_X, col_idx_to_Y]. So W_o[j,k] is W_o[e_out_idx, k]
    // This loop over embedding_dim (k) can be long. TDR risk.
    if (e_in_idx == 0 && instance_idx < batch_size * seq_len && e_out_idx < embedding_dim) { // Let one slice of e_in_idx compute this
        float sum_val = 0.0f;
        for (uint k = 0; k < embedding_dim; ++k) { // k iterates over the dimension W_o transforms into (which is also embedding_dim here)
            float dY_ik = float(grad_mhsa_output[instance_idx * embedding_dim + k]); 
            float W_jk = float(W_o[e_out_idx * embedding_dim + k]);                  // W_o[row_of_W_T = e_out_idx, col_of_W_T = k]
            sum_val += dY_ik * W_jk;
        }
        grad_concatenated_attention_heads[instance_idx * embedding_dim + e_out_idx] = half(sum_val);
    }
}

// Backward pass for Scaled Dot-Product Attention
kernel void scaled_dot_product_attention_backward(
    device const half* Q [[buffer(0)]],                 // [B, S, H, D] Query
    device const half* K [[buffer(1)]],                 // [B, S, H, D] Key
    device const half* V [[buffer(2)]],                 // [B, S, H, D] Value
    device const half* attention_weights [[buffer(3)]], // [B, H, S, S] Saved from forward pass
    device const half* grad_output [[buffer(4)]],       // [B, S, H, D] Gradient w.r.t. output
    device half* grad_Q_atomic_target [[buffer(5)]], // Target for atomic adds, cast to atomic_half if possible or use float atomics
    device half* grad_K [[buffer(6)]],                 // [B, S, H, D] Gradient w.r.t. K (output)
    device half* grad_V [[buffer(7)]],                 // [B, S, H, D] Gradient w.r.t. V (output)
    // If using half for atomics is not directly supported or buggy,
    // use device atomic_float* for grad_Q_atomic_target and cast at the end,
    // or implement atomic_half_add if necessary.
    // For simplicity, assuming grad_Q is pre-zeroed and we can do non-atomic read-modify-write
    // *if each grad_Q element is updated by only one logical summation pass*, but here it's not true.
    // So, grad_Q needs atomics. Let's use a float atomic buffer for grad_Q accumulation.
    device atomic_float* grad_Q_atomic_float [[buffer(12)]], // Assumed new buffer for float accumulation

    constant uint& batch_size [[buffer(8)]],
    constant uint& seq_len [[buffer(9)]],
    constant uint& num_heads [[buffer(10)]],
    constant uint& head_dim [[buffer(11)]],
    uint3 gid [[thread_position_in_grid]]  // gid.x = batch_idx, gid.y = head_idx, gid.z = output_seq_idx (target token for grad_K, grad_V)
) {
    uint batch_idx = gid.x;
    uint head_idx = gid.y;
    uint output_seq_idx = gid.z; // This is 'i' in some formulations for grad_K_i, grad_V_i
                                 // and 'k_idx' (key index) in others for Q_j K_k^T V_k
    
    if (batch_idx >= batch_size || head_idx >= num_heads || output_seq_idx >= seq_len) return;
    
    float scale = rsqrt(float(head_dim)); // 1.0f / sqrt(float(head_dim))
    
    // Initialize gradients for the elements this thread is primarily responsible for.
    // grad_K[b, output_seq_idx, h, d] and grad_V[b, output_seq_idx, h, d]
    // grad_Q is more complex as it's accumulated from different (output_seq_idx, attending_seq_idx) pairs.
    // It should be globally zeroed before the kernel launch.
    for (uint d = 0; d < head_dim; d++) {
        uint kv_idx_flat = ((batch_idx * seq_len + output_seq_idx) * num_heads + head_idx) * head_dim + d;
        // grad_Q is handled by atomic adds globally.
        grad_K[kv_idx_flat] = half(0.0f); // This thread will accumulate for this K
        grad_V[kv_idx_flat] = half(0.0f); // This thread will accumulate for this V
    }

    // Compute grad_V
    // grad_V[b,k,h,d] = sum_j (attn_weights[b,h,j,k] * grad_output[b,j,h,d])
    // Here, output_seq_idx is 'k'. We sum over 'j' (attending_seq_idx).
    for (uint d = 0; d < head_dim; ++d) {
        float sum_grad_v_d = 0.0f;
        for (uint attending_seq_idx_j = 0; attending_seq_idx_j < seq_len; ++attending_seq_idx_j) {
            // Causal mask: only attend to positions <= current query position for attention_weights[query_pos, key_pos]
            // The saved attention_weights should already incorporate masking.
            // attention_weights[batch, head, query_pos, key_pos]
            // Here, query_pos = attending_seq_idx_j, key_pos = output_seq_idx
            if (output_seq_idx > attending_seq_idx_j && false /* for strict causal in forward, check if needed here */) continue;


            uint attn_w_idx = ((batch_idx * num_heads + head_idx) * seq_len + attending_seq_idx_j) * seq_len + output_seq_idx;
            float attn_w = float(attention_weights[attn_w_idx]);

            uint grad_out_idx = ((batch_idx * seq_len + attending_seq_idx_j) * num_heads + head_idx) * head_dim + d;
            sum_grad_v_d += attn_w * float(grad_output[grad_out_idx]);
        }
        uint V_target_idx = ((batch_idx * seq_len + output_seq_idx) * num_heads + head_idx) * head_dim + d;
        grad_V[V_target_idx] = half(sum_grad_v_d);
    }

    // Compute grad_S (gradient of scores matrix before scaling)
    // grad_S[b,h,j,k] = (grad_attention_softmax[b,h,j,k] * scale)
    // where grad_attention_softmax = dS/dA * dA/dO * dL/dO -> dS/dA = softmax_backward_fn(A)
    // grad_attention_softmax[b,h,j,k] = (dL/dA_unscaled)[b,h,j,k] * attention_weights[b,h,j,k] (roughly, needs full softmax deriv)
    // More directly: grad_scores = grad_attention_weights_after_softmax_deriv * scale
    // Let dL/dA = grad_output @ V^T (where A is attention scores after softmax)
    // Then dL/dS = softmax_backward(dL/dA, A) (where S is pre-softmax scores QK^T)

    // Each thread gid(b,h,i) computes contributions to grad_Q[b,i,h,d] and grad_K[b,k,h,d]
    // Iterate over query_idx_j (attending_seq_idx) and key_idx_k (output_seq_idx which is gid.z)
    // This means a thread (b,h,k_fixed) computes all contributions *to* grad_K for this k_fixed,
    // and contributions *from* this k_fixed to various grad_Q.

    for (uint query_idx_j = 0; query_idx_j < seq_len; ++query_idx_j) {
        // This thread is for output_seq_idx = k_fixed.
        // We are calculating for an element of the attention matrix A[j, k_fixed]
        // A[j, k_fixed] = SoftmaxRow_j ( S[j,k_fixed] ) where S = QK^T/sqrt(d)
        // Masking: if k_fixed > query_idx_j for causal attention. Saved weights should handle this.
        uint key_idx_k = output_seq_idx; // Current thread's key index responsibility

        // if (key_idx_k > query_idx_j && true /* Causal mask applies */ ) continue; // Already baked into saved attention_weights

        // 1. Compute dL/dA_jk = sum_d (grad_output[b,j,h,d] * V[b,k,h,d])
        float dL_dA_jk = 0.0f; // Gradient of loss w.r.t. attention_weights[b,h,j,k]
        for (uint d = 0; d < head_dim; ++d) {
            uint grad_out_idx = ((batch_idx * seq_len + query_idx_j) * num_heads + head_idx) * head_dim + d;
            uint v_val_idx    = ((batch_idx * seq_len + key_idx_k) * num_heads + head_idx) * head_dim + d;
            dL_dA_jk += float(grad_output[grad_out_idx]) * float(V[v_val_idx]);
        }

        // 2. Compute dL/dS_jk (gradient of loss w.r.t. pre-softmax scaled scores)
        // dL/dS_row = (dL/dA_row - sum(dL/dA_row * A_row)) * A_row (element-wise)
        // Need sum_m (dL/dA_jm * A_jm) for the j-th row.
        float sum_dLdA_times_A_row_j = 0.0f;
        for (uint m = 0; m < seq_len; ++m) { // Iterate over columns of row j in attention matrix
             // Causal: if m > query_idx_j, A_jm is 0 or not considered.
             if (m > query_idx_j && true /* causal mask */) continue;

            float dL_dA_jm_temp = 0.0f;
            for (uint d_inner = 0; d_inner < head_dim; ++d_inner) {
                uint grad_out_idx_temp = ((batch_idx * seq_len + query_idx_j) * num_heads + head_idx) * head_dim + d_inner;
                uint v_val_idx_temp    = ((batch_idx * seq_len + m) * num_heads + head_idx) * head_dim + d_inner;
                dL_dA_jm_temp += float(grad_output[grad_out_idx_temp]) * float(V[v_val_idx_temp]);
            }
            uint attn_w_idx_jm = ((batch_idx * num_heads + head_idx) * seq_len + query_idx_j) * seq_len + m;
            sum_dLdA_times_A_row_j += dL_dA_jm_temp * float(attention_weights[attn_w_idx_jm]);
        }
        
        uint attn_w_idx_jk = ((batch_idx * num_heads + head_idx) * seq_len + query_idx_j) * seq_len + key_idx_k;
        float dL_dS_jk = (dL_dA_jk - sum_dLdA_times_A_row_j) * float(attention_weights[attn_w_idx_jk]);
        float grad_score_scaled = dL_dS_jk * scale; // dL/d(QK^T) = dL/dS_jk * scale

        // 3. Accumulate grad_Q and grad_K
        // grad_Q[j] += grad_score_scaled * K[k]
        // grad_K[k] += grad_score_scaled * Q[j]
        for (uint d = 0; d < head_dim; ++d) {
            uint q_target_idx = ((batch_idx * seq_len + query_idx_j) * num_heads + head_idx) * head_dim + d;
            uint k_val_idx    = ((batch_idx * seq_len + key_idx_k) * num_heads + head_idx) * head_dim + d;
            
            // CRITICAL: grad_Q needs atomic update as multiple (query_idx_j, key_idx_k) pairs from different threads
            // (different output_seq_idx/key_idx_k) might write to the same Q element if query_idx_j is the same.
            // Each thread (b,h,k_fixed) loops over query_idx_j.
            // If thread A (k_fixed=0) updates Q[j=0], and thread B (k_fixed=1) updates Q[j=0], this is a race.
            // Yes, this is a race on grad_Q.
            float term_for_grad_q = grad_score_scaled * float(K[k_val_idx]);
            atomic_fetch_add_explicit(&grad_Q_atomic_float[q_target_idx], term_for_grad_q, memory_order_relaxed);

            uint k_target_idx = ((batch_idx * seq_len + key_idx_k) * num_heads + head_idx) * head_dim + d;
            uint q_val_idx    = ((batch_idx * seq_len + query_idx_j) * num_heads + head_idx) * head_dim + d;
            // grad_K for key_idx_k is owned by this thread (gid.z = key_idx_k = output_seq_idx).
            // It accumulates contributions from all query_idx_j. So, serial accumulation is fine here.
            grad_K[k_target_idx] = half(float(grad_K[k_target_idx]) + grad_score_scaled * float(Q[q_val_idx]));
        }
    }
    // After kernel, copy from grad_Q_atomic_float to the actual half precision grad_Q buffer if needed.
}


// Backward pass for QKV Projection
kernel void qkv_projection_backward(
    device const half* input_X [[buffer(0)]],             // [B, S, E] Input embeddings (X)
    device const half* qkv_weights [[buffer(1)]],         // [3*E, E] Concatenated W_q,W_k,W_v (Layout: W[input_dim_to_W_qkv, output_dim_from_W_qkv])
                                                          // Assuming W_qkv has shape (E_in, 3*E_out) where E_in=E, E_out=E for Q,K,V each.
                                                          // So qkv_weights is (E, 3E). To make it [3E,E] it would be (W_qkv)^T
                                                          // Let's assume qkv_weights is (E_in_to_W, 3*E_out_from_W) = (E, 3E)
                                                          // W_q: w[e_in, 0*E + e_out_q]
                                                          // W_k: w[e_in, 1*E + e_out_k]
                                                          // W_v: w[e_in, 2*E + e_out_v]
    device const half* grad_qkv_output [[buffer(2)]],     // [B, S, 3*E] Gradient w.r.t. QKV output (dL/d(QKV))
    
    device half* grad_input_X [[buffer(3)]],              // [B, S, E] Gradient w.r.t. input X (dL/dX)
    // These must be atomic if multiple threads sum into them, or pre-zeroed.
    // If pre-zeroed and using atomics:
    device atomic_float* grad_qkv_weights_atomic [[buffer(4)]], // [E, 3*E] (atomic)
    device atomic_float* grad_qkv_bias_atomic [[buffer(5)]],    // [3*E] (atomic) - Assuming bias was used in forward. If not, this is unused.

    constant uint& batch_size [[buffer(6)]],
    constant uint& seq_len [[buffer(7)]],
    constant uint& embedding_dim [[buffer(8)]],           // E (input dimension to QKV projection and output dimension for each Q,K,V head)

    // Grid dispatch: (instance_idx, e_in_idx, qkv_out_idx_component)
    // instance_idx: 0 to B*S - 1
    // e_in_idx: 0 to E - 1 (input embedding dimension of X, also row index for W_qkv)
    // qkv_out_idx_global: 0 to 3*E - 1 (output dimension of QKV combined, also col index for W_qkv)
    uint3 gid [[thread_position_in_grid]]
) {
    uint instance_idx = gid.x;
    uint e_in_idx = gid.y;          // Dimension of X, and row index of W_qkv
    uint qkv_out_idx_global = gid.z; // Flattened output dimension of QKV matrix (0 to 3E-1), and col index of W_qkv

    uint total_instances = batch_size * seq_len;

    // --- Calculate dL/dX (grad_input_X) ---
    // dL/dX[inst, e_in] = sum_{j=0 to 3E-1} ( grad_QKV[inst, j] * W_qkv[e_in, j] )
    // Each thread (instance_idx, e_in_idx, qkv_out_idx_global) calculates one term of the sum for dL/dX.
    // This requires reduction over qkv_out_idx_global for each (instance_idx, e_in_idx).
    // This specific gid structure is not ideal for directly computing one dL/dX element.
    // Let's assume for now that dL/dX is computed by threads responsible for (instance_idx, e_in_idx)
    // and they loop over qkv_out_idx_global. This can cause TDR if 3*E is large.
    // The original gid was (computation_type, ...). This restructuring means the host must dispatch differently.
    if (instance_idx < total_instances && e_in_idx < embedding_dim && qkv_out_idx_global == 0) { // Let qkv_out_idx_global=0 thread do the sum
        float sum_for_grad_input_x = 0.0f;
        for (uint j = 0; j < 3 * embedding_dim; ++j) {
            float grad_qkv_val = float(grad_qkv_output[instance_idx * (3 * embedding_dim) + j]);
            // qkv_weights[row, col] = qkv_weights[e_in_idx_W, j_W]
            float weight_val = float(qkv_weights[e_in_idx * (3 * embedding_dim) + j]);
            sum_for_grad_input_x += grad_qkv_val * weight_val;
        }
        grad_input_X[instance_idx * embedding_dim + e_in_idx] = half(sum_for_grad_input_x);
    }

    // --- Calculate dL/dW_qkv ---
    // dL/dW[e_in, qkv_out] = sum_{inst} ( X[inst, e_in] * grad_QKV[inst, qkv_out] )
    // Each thread (instance_idx, e_in_idx, qkv_out_idx_global) calculates one term of the sum.
    // Add this term atomically to grad_qkv_weights_atomic[e_in, qkv_out].
    if (instance_idx < total_instances && e_in_idx < embedding_dim && qkv_out_idx_global < 3 * embedding_dim) {
        float x_val = float(input_X[instance_idx * embedding_dim + e_in_idx]);
        float grad_qkv_val = float(grad_qkv_output[instance_idx * (3 * embedding_dim) + qkv_out_idx_global]);
        
        uint weight_offset = e_in_idx * (3 * embedding_dim) + qkv_out_idx_global;
        atomic_fetch_add_explicit(&grad_qkv_weights_atomic[weight_offset], x_val * grad_qkv_val, memory_order_relaxed);
    }

    // --- Calculate dL/db_qkv ---
    // dL/db[qkv_out] = sum_{inst} ( grad_QKV[inst, qkv_out] )
    // Each thread (instance_idx, e_in_idx=0, qkv_out_idx_global) contributes.
    // e_in_idx=0 is used to ensure each instance's grad_QKV value is added once per qkv_out_idx_global.
    if (instance_idx < total_instances && e_in_idx == 0 && qkv_out_idx_global < 3 * embedding_dim) {
        float grad_qkv_val = float(grad_qkv_output[instance_idx * (3 * embedding_dim) + qkv_out_idx_global]);
        atomic_fetch_add_explicit(&grad_qkv_bias_atomic[qkv_out_idx_global], grad_qkv_val, memory_order_relaxed);
    }
}


// Backward pass for Embedding Layer
kernel void embedding_layer_backward(
    device const uint32_t* token_ids [[buffer(0)]],         // [B, S] - Token IDs from forward pass
    device const half* grad_output_embeddings [[buffer(1)]],   // [B, S, E] - Gradient w.r.t. output embeddings
    device atomic_float* grad_embedding_table [[buffer(2)]],   // [V, E] - Gradient w.r.t. embedding table (output, atomic accumulation)
    constant uint& batch_size [[buffer(3)]],
    constant uint& seq_len [[buffer(4)]],
    constant uint& vocab_size [[buffer(5)]],
    constant uint& embedding_dim [[buffer(6)]],
    constant uint& pad_token_id [[buffer(7)]],
    // gid.x = instance_idx (0 to B*S - 1)
    // gid.y = e_idx (0 to E - 1)
    uint2 gid [[thread_position_in_grid]]
) {
    uint instance_idx = gid.x; 
    uint e_idx = gid.y;
    
    if (instance_idx >= batch_size * seq_len || e_idx >= embedding_dim) return;
    
    uint32_t token_id_val = token_ids[instance_idx]; // Corrected from token_idx to instance_idx
    
    if (token_id_val == pad_token_id || token_id_val >= vocab_size) return;
    
    uint grad_output_offset = instance_idx * embedding_dim + e_idx;
    uint grad_embedding_offset = token_id_val * embedding_dim + e_idx;
    
    float grad_val = float(grad_output_embeddings[grad_output_offset]);
    atomic_fetch_add_explicit(&grad_embedding_table[grad_embedding_offset], grad_val, memory_order_relaxed);
}

// Utility kernel: Extract separate Q, K, V from concatenated QKV buffer
kernel void extract_qkv_from_concatenated(
    device const half* concatenated_qkv [[buffer(0)]],   // [B, S, 3*E] Concatenated QKV
    device half* Q [[buffer(1)]],                       // [B, S, H, D] Query output
    device half* K [[buffer(2)]],                       // [B, S, H, D] Key output  
    device half* V [[buffer(3)]],                       // [B, S, H, D] Value output
    constant uint& batch_size [[buffer(4)]],
    constant uint& seq_len [[buffer(5)]],
    constant uint& embedding_dim [[buffer(6)]],         // E
    constant uint& num_heads [[buffer(7)]],             // H
    // gid.x = instance_idx (0 to B*S - 1)
    // gid.y = head_idx (0 to H - 1)
    // gid.z = dim_in_head_idx (0 to D - 1)
    uint3 gid [[thread_position_in_grid]]
) {
    uint instance_idx = gid.x; // Combined batch and sequence index
    uint head_idx = gid.y;
    uint dim_in_head_idx = gid.z;
    
    if (instance_idx >= batch_size * seq_len || head_idx >= num_heads) return;
    
    uint head_dim = embedding_dim / num_heads;
    if (head_dim == 0 || dim_in_head_idx >= head_dim) return; // Avoid division by zero and out of bounds
    
    // Offset in the source concatenated_qkv buffer [B, S, 3*E]
    // Input is effectively [B*S, 3*E]. Each token has its Q, K, V parts concatenated.
    // For a given token (instance_idx):
    // Q elements are at [instance_idx * 3E + 0*E + local_e_idx]
    // K elements are at [instance_idx * 3E + 1*E + local_e_idx]
    // V elements are at [instance_idx * 3E + 2*E + local_e_idx]
    // where local_e_idx is (head_idx * head_dim + dim_in_head_idx)
    uint local_e_idx = head_idx * head_dim + dim_in_head_idx;
    if (local_e_idx >= embedding_dim) return; // Should not happen if head_idx/dim_in_head_idx are correct

    uint qkv_base_offset_for_token = instance_idx * (3 * embedding_dim);
    
    // Offset in the destination Q, K, V buffers [B, S, H, D]
    // Output is effectively [B*S, H, D]
    uint output_offset = (instance_idx * num_heads + head_idx) * head_dim + dim_in_head_idx;
    
    Q[output_offset] = concatenated_qkv[qkv_base_offset_for_token + 0 * embedding_dim + local_e_idx];
    K[output_offset] = concatenated_qkv[qkv_base_offset_for_token + 1 * embedding_dim + local_e_idx];
    V[output_offset] = concatenated_qkv[qkv_base_offset_for_token + 2 * embedding_dim + local_e_idx];
}


// Enhanced Scaled Dot-Product Attention with Attention Weights Saving (Forward Pass)
kernel void scaled_dot_product_attention_with_weights_save(
    device const half* qkv_input [[buffer(0)]], // [B,S,3E] Concatenated Q,K,V for all tokens from a projection layer
    device half* attention_output [[buffer(1)]],    // [B,S,E] Output, heads concatenated
    device half* attention_weights [[buffer(2)]],   // [B,H,S,S] Saved attention weights
    constant uint& batch_size [[buffer(3)]],
    constant uint& sequence_length [[buffer(4)]],   // S
    constant uint& embedding_dim [[buffer(5)]],     // E (total embedding dim, num_heads * head_dim)
    constant uint& num_heads [[buffer(6)]],         // H
    // gid.x = batch_idx
    // gid.y = head_idx
    // gid.z = query_seq_idx (seq_i)
    uint3 gid [[thread_position_in_grid]]
) {
    uint batch_idx = gid.x;
    uint head_idx = gid.y;
    uint query_seq_idx = gid.z; // This thread computes one row of the attention matrix for this head
    
    if (batch_idx >= batch_size || head_idx >= num_heads || query_seq_idx >= sequence_length) return;
    
    uint head_dim = embedding_dim / num_heads;
    if (head_dim == 0) return;
    float scale = rsqrt(float(head_dim));
    
    // --- For current (batch_idx, head_idx, query_seq_idx) ---
    float max_score = -INFINITY;
    
    // 1. Calculate scores and find max_score for softmax numerical stability (for current query_seq_idx row)
    // Q is q_input[batch_idx, query_seq_idx, head_idx, d]
    // K is k_input[batch_idx, key_seq_idx,  head_idx, d]
    // qkv_input stores Q,K,V interleaved: Q1K1V1Q2K2V2... no, it's token1_QKV, token2_QKV
    // So for token `s`: qkv_input[s*3E ... (s+1)*3E-1]
    // Q for token `s`, head `h`, dim `d_h`: qkv_input[s*3E + 0*E + h*D + d_h]
    // K for token `s`, head `h`, dim `d_h`: qkv_input[s*3E + 1*E + h*D + d_h]
    // V for token `s`, head `h`, dim `d_h`: qkv_input[s*3E + 2*E + h*D + d_h]

    uint q_token_base = (batch_idx * sequence_length + query_seq_idx) * (3 * embedding_dim);
    uint q_head_offset_in_token = 0 * embedding_dim + head_idx * head_dim;

    // Temporary array for scores for the current query_seq_idx row
    // Max sequence_length for this stack array needs to be managed.
    // For very large sequence_length, threadgroup memory or device memory might be needed.
    float scores_row[1024]; // Assuming sequence_length <= 1024
    if (sequence_length > 1024) return; // Safety break for stack array

    for (uint key_seq_idx = 0; key_seq_idx < sequence_length; ++key_seq_idx) {
        // Apply causal mask: can only attend to past keys (key_seq_idx <= query_seq_idx)
        if (key_seq_idx > query_seq_idx) {
            scores_row[key_seq_idx] = -INFINITY; // Masked out scores
            continue;
        }

        float score = 0.0f;
        uint k_token_base = (batch_idx * sequence_length + key_seq_idx) * (3 * embedding_dim);
        uint k_head_offset_in_token = 1 * embedding_dim + head_idx * head_dim;

        for (uint d = 0; d < head_dim; ++d) {
            float q_val = float(qkv_input[q_token_base + q_head_offset_in_token + d]);
            float k_val = float(qkv_input[k_token_base + k_head_offset_in_token + d]);
            score += q_val * k_val;
        }
        score *= scale;
        scores_row[key_seq_idx] = score;
        max_score = max(max_score, score);
    }
    
    // 2. Compute sum_exp for softmax denominator
    float sum_exp_scores = 0.0f;
    for (uint key_seq_idx = 0; key_seq_idx < sequence_length; ++key_seq_idx) {
        if (key_seq_idx > query_seq_idx) continue; // Respect causal mask
        sum_exp_scores += exp(scores_row[key_seq_idx] - max_score);
    }

    // 3. Compute attention weights and output value
    // attention_weights[batch, head, query_pos, key_pos]
    // attention_output[batch, query_pos, head_concat_dim]
    // Output for current token (query_seq_idx), current head (head_idx)
    uint output_instance_offset = (batch_idx * sequence_length + query_seq_idx) * embedding_dim;
    uint output_head_start_dim = head_idx * head_dim;

    for (uint d_out = 0; d_out < head_dim; ++d_out) {
        float output_val_d = 0.0f;
        for (uint key_seq_idx = 0; key_seq_idx < sequence_length; ++key_seq_idx) {
            if (key_seq_idx > query_seq_idx) continue; // Respect causal mask

            float attn_weight_val = exp(scores_row[key_seq_idx] - max_score) / sum_exp_scores;
            
            // Save attention weight (only one thread along d_out needs to do this)
            if (d_out == 0) {
                uint attn_w_save_idx = ((batch_idx * num_heads + head_idx) * sequence_length + query_seq_idx) * sequence_length + key_seq_idx;
                attention_weights[attn_w_save_idx] = half(attn_weight_val);
            }
            
            uint v_token_base = (batch_idx * sequence_length + key_seq_idx) * (3 * embedding_dim);
            uint v_head_offset_in_token = 2 * embedding_dim + head_idx * head_dim;
            output_val_d += attn_weight_val * float(qkv_input[v_token_base + v_head_offset_in_token + d_out]);
        }
        attention_output[output_instance_offset + output_head_start_dim + d_out] = half(output_val_d);
    }
}


// Inference-specific kernel: QKV Projection for a single token
kernel void qkv_projection_inference(
    device const half* current_token_embedding [[buffer(0)]], // [E] - Embedding of the current token
    device const half* qkv_weights [[buffer(1)]],             // [E, 3*E] - Combined QKV projection weights (W_q; W_k; W_v transposed and stacked, or W_qkv as (E_in, 3*E_out))
                                                              // Assuming qkv_weights is (E_in_dim_of_W, 3*E_out_dim_of_W) = (E, 3E)
    device const half* qkv_bias [[buffer(2)]],                // [3*E] - Combined QKV projection bias
    device half* Q_current [[buffer(3)]],                     // [E] - Output Q for the current token
    device half* K_current [[buffer(4)]],                     // [E] - Output K for the current token
    device half* V_current [[buffer(5)]],                     // [E] - Output V for the current token
    constant uint& embedding_dim [[buffer(6)]],
    uint out_e_idx [[thread_position_in_grid]] // gid.x over one of the E dimensions of Q,K,V (0 to E-1)
) {
    if (out_e_idx >= embedding_dim) return;

    // Q_current[out_e_idx] = sum_e (current_token_embedding[e] * W_q[e, out_e_idx]) + b_q[out_e_idx]
    // W_q is the first E x E block of qkv_weights. W_q[row,col] = qkv_weights[row * (3*embedding_dim) + (0*embedding_dim + col)] if E_rows, 3E_cols
    // If qkv_weights is (E_rows, 3E_cols): W_q is qkv_weights[row_idx, 0*E + out_e_idx]
    
    float sum_q = float(qkv_bias[0 * embedding_dim + out_e_idx]);
    for (uint e_in = 0; e_in < embedding_dim; ++e_in) {
        sum_q += float(current_token_embedding[e_in]) * float(qkv_weights[e_in * (3 * embedding_dim) + (0 * embedding_dim + out_e_idx)]);
    }
    Q_current[out_e_idx] = half(sum_q);

    float sum_k = float(qkv_bias[1 * embedding_dim + out_e_idx]);
    for (uint e_in = 0; e_in < embedding_dim; ++e_in) {
        sum_k += float(current_token_embedding[e_in]) * float(qkv_weights[e_in * (3 * embedding_dim) + (1 * embedding_dim + out_e_idx)]);
    }
    K_current[out_e_idx] = half(sum_k);

    float sum_v = float(qkv_bias[2 * embedding_dim + out_e_idx]);
    for (uint e_in = 0; e_in < embedding_dim; ++e_in) {
        sum_v += float(current_token_embedding[e_in]) * float(qkv_weights[e_in * (3 * embedding_dim) + (2 * embedding_dim + out_e_idx)]);
    }
    V_current[out_e_idx] = half(sum_v);
}


// Inference-specific kernel: Scaled Dot-Product Attention with KV Caching
kernel void scaled_dot_product_attention_inference(
    device const half* Q_current_token_all_heads [[buffer(0)]], // [E] - Q for current token, all heads concatenated
    device const half* K_current_token_all_heads [[buffer(1)]], // [E] - K for current token, all heads concatenated
    device const half* V_current_token_all_heads [[buffer(2)]], // [E] - V for current token, all heads concatenated
    device half* kv_cache_K_layer [[buffer(3)]],      // [H, MaxS, Dh] - K cache for the current layer (flattened)
    device half* kv_cache_V_layer [[buffer(4)]],      // [H, MaxS, Dh] - V cache for the current layer (flattened)
    device half* attention_output_all_heads [[buffer(5)]], // [E] - Output for current token, all heads concatenated
    constant uint& current_token_idx [[buffer(6)]],      // 0-indexed position of the current token in the sequence
    constant uint& embedding_dim [[buffer(7)]],
    constant uint& num_heads [[buffer(8)]],
    constant uint& max_sequence_length [[buffer(9)]],  // Max capacity of the cache
    // gid.x = head_idx (0 to num_heads - 1)
    // gid.y = d_in_head_idx (0 to head_dim - 1) for parallelizing head computation
    uint2 gid [[thread_position_in_grid]] 
) {
    uint head_idx = gid.x;
    uint d_in_head_idx = gid.y; // Thread for one dimension within the head's output vector

    if (head_idx >= num_heads) return;

    uint head_dim = embedding_dim / num_heads;
    if (head_dim == 0 || d_in_head_idx >= head_dim) return;
    
    float scale = rsqrt(float(head_dim));

    // --- 1. Update KV Cache ---
    // This part can be done by fewer threads if d_in_head_idx is for the matmul later.
    // Let's assume one thread d_in_head_idx=0 for each head_idx updates its part of the cache.
    if (d_in_head_idx < head_dim) { // All threads in head_dim range can write their part of K/V to cache.
        uint cache_base_offset_for_head = head_idx * max_sequence_length * head_dim;
        uint cache_offset_current_token_in_head = cache_base_offset_for_head + current_token_idx * head_dim;

        if (current_token_idx < max_sequence_length) {
            uint current_K_offset_in_E = head_idx * head_dim + d_in_head_idx;
            kv_cache_K_layer[cache_offset_current_token_in_head + d_in_head_idx] = K_current_token_all_heads[current_K_offset_in_E];
            
            uint current_V_offset_in_E = head_idx * head_dim + d_in_head_idx;
            kv_cache_V_layer[cache_offset_current_token_in_head + d_in_head_idx] = V_current_token_all_heads[current_V_offset_in_E];
        }
    }
    // All threads in the threadgroup working on the same head_idx need to see these writes.
    threadgroup_barrier(mem_flags::mem_device); // Ensure cache writes are visible to all threads/threadgroups.
                                                // mem_threadgroup if threads in one group share this head.

    // --- 2. Compute Attention Scores for Q_current with all K in cache for this head ---
    // Each thread (head_idx, d_in_head_idx=0) can compute all scores for its head.
    // Or, parallelize score computation if sequence_length_so_far is large.
    // For now, thread d_in_head_idx=0 of each head computes scores and softmax.
    
    threadgroup float tg_attention_scores[1024]; // MaxS for this head. Max 1024.
    threadgroup float tg_max_score;
    threadgroup float tg_sum_exp_scores;

    if (max_sequence_length > 1024) return; // Safety for threadgroup array

    uint sequence_length_so_far = current_token_idx + 1;
    if (sequence_length_so_far > 1024) return;


    if (d_in_head_idx == 0) { // Master thread for this head calculates scores and softmax params
        tg_max_score = -INFINITY;
        uint cache_base_offset_for_head = head_idx * max_sequence_length * head_dim;
        uint q_current_head_offset = head_idx * head_dim;

        for (uint k_token_cache_idx = 0; k_token_cache_idx < sequence_length_so_far; ++k_token_cache_idx) {
            float score = 0.0f;
            uint k_cached_token_offset_in_head = cache_base_offset_for_head + k_token_cache_idx * head_dim;
            for (uint d_score = 0; d_score < head_dim; ++d_score) {
                score += float(Q_current_token_all_heads[q_current_head_offset + d_score]) *
                         float(kv_cache_K_layer[k_cached_token_offset_in_head + d_score]);
            }
            score *= scale;
            tg_attention_scores[k_token_cache_idx] = score;
            tg_max_score = max(tg_max_score, score);
        }
        
        tg_sum_exp_scores = 0.0f;
        for (uint k_token_cache_idx = 0; k_token_cache_idx < sequence_length_so_far; ++k_token_cache_idx) {
            float exp_val = exp(tg_attention_scores[k_token_cache_idx] - tg_max_score);
            tg_attention_scores[k_token_cache_idx] = exp_val; // Store unnormalized exp for now
            tg_sum_exp_scores += exp_val;
        }
    }
    threadgroup_barrier(mem_flags::mem_threadgroup); // Ensure scores, max_score, sum_exp are computed.

    // --- 3. Compute Output: weighted sum of V ---
    // Each thread (head_idx, d_in_head_idx) computes one dimension of the output vector for its head.
    float output_val_d = 0.0f;
    uint cache_base_offset_for_head = head_idx * max_sequence_length * head_dim;

    for (uint k_token_cache_idx = 0; k_token_cache_idx < sequence_length_so_far; ++k_token_cache_idx) {
        float softmax_weight = tg_attention_scores[k_token_cache_idx] / tg_sum_exp_scores;
        uint v_cached_token_offset_in_head = cache_base_offset_for_head + k_token_cache_idx * head_dim;
        output_val_d += softmax_weight * float(kv_cache_V_layer[v_cached_token_offset_in_head + d_in_head_idx]);
    }
    
    uint output_E_offset = head_idx * head_dim + d_in_head_idx;
    attention_output_all_heads[output_E_offset] = half(output_val_d);
}