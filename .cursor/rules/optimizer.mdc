---
description: Cursorrules for Optimizer (.cursor/rules/optimizer.mdc)
globs: 
alwaysApply: false
---
// Cursorrules for Optimizer (.cursor/rules/optimizer.mdc)

// Phase: 3 - Model Assembly & Training Orchestration
// Component: Task 3.3 Training Loop (Optimizer Step part)

// Rule: Optimizer Choice - Adam/AdamW Default
// Description: Implement the Adam or AdamW optimizer. AdamW is generally preferred for Transformers due to better weight decay handling.
// The optimizer updates model parameters using their computed gradients.
// Applies to: MSL kernel code, Host C++ code.

// Rule: Optimizer State Buffers
// Description: Adam/AdamW requires maintaining optimizer state for each parameter: first moment vector (m) and second moment vector (v).
// These state vectors must be stored in `MTLBuffer`s, with the same dimensions as their corresponding parameters. Allocate with `MTLStorageModeShared`.
// Data type: `float` is strongly recommended for moment vectors for stability, even if parameters/gradients are `half`.
// Applies to: Host C++ code (buffer allocation), MSL kernel signatures.

// Rule: Kernel Function Signature (AdamW Update)
// Description: An MSL kernel will perform the AdamW update for each parameter.
// Example: `kernel void adamw_update(device half* param [[buffer(0)]], device const grad_type* grad [[buffer(1)]], device float* m_state [[buffer(2)]], device float* v_state [[buffer(3)]], constant AdamWConfig cfg [[buffer(4)]], constant uint t [[buffer(5)]] /* timestep */, uint gid [[thread_position_in_grid]])` where `grad_type` can be `half` or `float`.
// `AdamWConfig` would be a struct with learning_rate, beta1, beta2, epsilon, weight_decay.
// Applies to: MSL kernel code.

// Rule: AdamW Update Logic (Per Parameter Element)
// Description: For each element of a parameter `p` with gradient `g`:
//  1. `t = t + 1` (timestep, incremented globally per optimizer step).
//  2. `m_state = beta1 * m_state + (1 - beta1) * g`
//  3. `v_state = beta2 * v_state + (1 - beta2) * g*g`
//  4. `m_hat = m_state / (1 - beta1^t)` (bias correction for 1st moment)
//  5. `v_hat = v_state / (1 - beta2^t)` (bias correction for 2nd moment)
//  6. `p = p - learning_rate * (m_hat / (sqrt(v_hat) + epsilon))` (Adam part)
//  7. `p = p - learning_rate * weight_decay * p` (AdamW weight decay part, applied directly to weight `p` *after* the Adam update, or as `g = g + weight_decay * p` before step 2 if following some interpretations).
// All calculations involving moments (`m_state`, `v_state`, `m_hat`, `v_hat`) and the update step should use `float` precision.
// Applies to: MSL kernel code.

// Rule: Hyperparameters
// Description: Learning rate, beta1, beta2, epsilon, weight_decay are hyperparameters. These should be configurable (e.g., via `TransformerConfig` or a separate optimizer config) and passed to the MSL kernel, likely via a constant buffer.
// Common defaults: lr (e.g., 1e-4 to 5e-5), beta1=0.9, beta2=0.999 (or 0.98 for some Transformer setups), epsilon=1e-8 (or 1e-6), weight_decay (e.g., 0.01 or 0.1).
// Applies to: Host C++ code (config), MSL kernel code.

// Rule: Learning Rate Scheduler
// Description: Implement learning rate scheduling (e.g., linear warmup followed by linear or cosine decay). The host C++ code will calculate the current learning rate based on the current training step/epoch and pass it to the optimizer kernel.
// Applies to: Host C++ code.

// Rule: Optimizer Step Orchestration (Host side)
// Description: The host code, after all gradients are computed, will dispatch the optimizer kernel(s) for each model parameter, providing the parameter buffer, its gradient buffer, its state buffers (m, v), and hyperparameters.
// Applies to: Host C++ code.

// Rule: Gradient Clipping (Optional)
// Description: If gradients are exploding, implement gradient clipping (e.g., by norm) before the optimizer step. This would typically be an additional MSL kernel that iterates over all gradient buffers, calculates the global norm, and rescales gradients if the norm exceeds a threshold.
// Applies to: MSL kernel code (if implemented), Host C++ code.

// Rule: Data Types for Parameters and Gradients
// Description: Parameters can be `half`. Gradients can be `half` or `float`. Optimizer states (m, v) should be `float`. The MSL kernel must handle potential type conversions (e.g., `half` grad to `float` for moment updates).
// Using `half` for gradients saves bandwidth but requires careful handling in `float` moment updates.
// Applies to: MSL kernel code.

// Rule: Testing (TDD) - Optimizer Kernel
// Description:
//  - Prepare a small parameter tensor (`half` or `float`) with known initial values.
//  - Prepare a corresponding gradient tensor (`half` or `float`) with known values.
//  - Initialize optimizer state buffers (m, v) to zeros (`float`).
//  - Set known hyperparameters (lr, beta1, beta2, epsilon, weight_decay) and timestep `t`.
//  - Manually (or with Python/PyTorch `torch.optim.AdamW`) calculate the expected updated parameter values and the expected updated m and v states after one step.
//  - Execute the MSL adamw_update kernel.
//  - Read back the updated parameter buffer and state buffers (m, v) from GPU.
//  - Compare with expected values (element-wise with tolerance, especially for `float` states and potentially `half` params).
//  - Test for several steps to ensure bias correction and moment accumulation behave as expected.
//  - Test the learning rate scheduling logic on the CPU side: verify the correct learning rate is computed for different training steps.
//  - Test gradient clipping kernel separately if implemented.
// Applies to: MSL kernel code, Host C++/Swift test harness.
