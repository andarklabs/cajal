---
description: Cursorrules for Backward Pass (Autodiff) (.cursor/rules/backward_pass_autodiff.mdc)
globs: 
alwaysApply: false
---
// Cursorrules for Backward Pass (Autodiff) (.cursor/rules/backward_pass_autodiff.mdc)

// Phase: 3 - Model Assembly & Training Orchestration
// Component: Task 3.3 Training Loop (Backward Pass / Autodiff part)

// Rule: Overall Principle - Chain Rule in Reverse
// Description: The backward pass implements reverse-mode automatic differentiation (autodiff) by applying the chain rule iteratively, starting from the gradient of the loss with respect to the model's output (logits), and propagating gradients back through each layer/operation to compute gradients for all parameters and inputs to layers.
// Applies to: Overall design of backward pass, MSL gradient kernels.

// Rule: Gradient Buffer Management
// Description: For every model parameter (weight/bias `MTLBuffer`) that requires gradients, a corresponding `MTLBuffer` of the same dimensions must be allocated to store its gradients (e.g., `dWq`, `db1`). Similarly, for every activation `MTLBuffer` passed between layers in the forward pass, a corresponding gradient buffer (`dActivation`) will be needed to pass gradients backward between layers.
// All gradient buffers should use `MTLStorageModeShared` and typically `half` or `float` precision matching the parameters/activations.
// Applies to: Host C++ code (buffer allocation), MSL kernel signatures.

// Rule: Gradient Accumulation
// Description: Gradients for parameters (weights/biases) are accumulated across the batch. Ensure gradient buffers for parameters are zeroed out before the start of a new batch's backward pass. The optimizer step uses these accumulated gradients.
// Applies to: Host C++ code (gradient zeroing), MSL gradient kernels (some might do read-modify-write if adding to existing gradients, though often output is written directly if zeroed before).

// Rule: Sequence of Gradient Kernels
// Description: The backward pass is a sequence of MSL kernels, executed in the reverse order of the forward pass operations. Each kernel computes the gradients for the inputs of its corresponding forward operation and the gradients for the parameters of that operation, given the gradients from the operation that followed it in the forward pass.
// Example Order: dLoss/dLogits -> dOutputLayer -> dAddNorm_N -> dFFN_N -> dAddNorm_N-1 -> dAttention_N -> ... -> dEmbeddingLayer.
// Applies to: Host C++ code (orchestration of backward pass).

// Rule: Gradient Kernels - General Structure
// Description: Each gradient kernel will typically take:
//  1. `grad_output`: The gradient from the subsequent layer/operation (what we are backpropagating).
//  2. `activations_input`: Necessary activations saved from the forward pass of the *current* operation (e.g., input tensor to a linear layer, attention scores).
//  3. `parameters`: Weights/biases of the current operation (if it has them).
//  And will output:
//  1. `grad_input`: Gradient w.r.t. the input of the current operation (to be passed further backward).
//  2. `grad_parameters`: Gradients w.r.t. the parameters of the current operation (e.g., dW, db).
// Applies to: MSL gradient kernel design.

// Rule: Time Efficiency - Fused Kernels for Gradients
// Description: Where possible, fuse gradient calculations. For example, the gradient calculation for a linear layer (`Y = XW + b`) involves `dX = dY @ W^T`, `dW = X^T @ dY`, `db = sum(dY)`. If `dY` is available, `dX` and `dW` could potentially be computed in a more fused manner to improve data reuse and reduce kernel launch overhead compared to three separate kernels.
// Prioritize reducing memory bandwidth by keeping intermediate gradient values in registers or threadgroup memory.
// Applies to: MSL gradient kernel design, Optimization efforts.

// Rule: Time Efficiency - Optimized Matrix Multiplications
// Description: The backward pass heavily relies on matrix multiplications (e.g., `dY @ W^T`, `X^T @ dY`). These must be highly optimized using `simdgroup_matrix`, `threadgroup_matrix` primitives, or efficient manual blocking in MSL, similar to forward pass matmuls.
// Data layouts (transpositions) will be critical for performance.
// Applies to: MSL gradient kernel design (especially for linear layers, attention gradients).

// Rule: Time Efficiency - Data Types for Gradients
// Description: Use `half` precision for gradients where possible, especially for `dActivation` buffers, to save memory and bandwidth. Parameter gradients (`dW`, `db`) might also be `half` if the optimizer can handle it, or accumulated in `float` then cast if necessary. This is a trade-off between speed/memory and precision for optimizer stability.
// Default to matching the precision of the corresponding forward pass buffer and parameter, but explore `half` aggressively.
// Applies to: MSL gradient kernel design, Host C++ buffer allocation.

// Rule: Specific Gradient Kernel Design (High-Level - details in layer-specific .mdc or during implementation)
// Description:
//  - **dLoss/dLogits**: `softmax(logits) - one_hot_targets`. (Often computed alongside loss).
//  - **dOutputLayer (Linear)**: Given `dLogits`, compute `dHidden_N`, `dW_out`, `db_out`.
//  - **dAddNorm**: Given `dOutput_norm`, backprop through LayerNorm (complex, involves dgamma, dbeta, and splitting grad for residual and sublayer_output paths) and Add operation (simple gradient pass-through/split).
//  - **dFFN (Linear -> Activation -> Linear)**: Backpropagate through second linear, then activation derivative, then first linear. Compute gradients for FFN weights/biases.
//  - **dAttention (MHSA)**: Most complex. Backprop through output projection, then scaled dot-product attention (gradients for Q, K, V involving attention weights), then QKV projection (gradients for Wq, Wk, Wv and input to attention).
//  - **dPositionalEncoding**: If PEs are added, gradient is passed through. If learned, it's like dEmbedding.
//  - **dEmbedding**: Given `dEmbedOutput`, gradient for embedding weights is a scatter-add operation based on input token IDs.
// Applies to: MSL gradient kernel design for each layer type.

// Rule: Storing Activations from Forward Pass
// Description: The forward pass must save any activations required for the backward pass (e.g., input to a linear layer for `dW` calculation, input to LayerNorm, attention scores for attention gradient). These are typically stored in `MTLBuffer`s.
// Balance memory cost of storing activations vs. recomputing them (gradient checkpointing - advanced).
// Applies to: Host C++ (forward pass logic), MSL forward kernels.

// Rule: Gradient Checkpointing (Advanced Optimization)
// Description: For memory-intensive models, instead of storing all activations, recompute some activations during the backward pass. This trades compute for memory. This is an advanced technique to consider if memory limits are hit.
// Applies to: Advanced optimization strategy.

// Rule: Testing (TDD) - Backward Pass and Gradient Kernels
// Description: This is critical and complex. Test each gradient kernel individually and meticulously.
//  - **General Strategy for each `dOperation` kernel:**
//    1. Perform a forward pass of the original operation with small, known inputs (activations, weights) and get its output. Use a trusted CPU/PyTorch version of the operation as a reference.
//    2. Assume a small, known `grad_output` (gradient for the operation's output).
//    3. Manually (or using PyTorch `.backward()` on the reference operation) calculate the expected `grad_input` and `grad_parameters`.
//    4. Execute the MSL `dOperation` kernel with the known `grad_output`, saved forward pass activations, and parameters.
//    5. Read back the computed `grad_input` and `grad_parameters` from GPU and compare with expected values (element-wise with tolerance).
//  - **Test data types:** Ensure `half`/`float` precision choices are handled correctly and comparisons use appropriate tolerances.
//  - **Test accumulation:** For parameter gradients, test that they accumulate correctly over multiple (mock) batches if not zeroed each time (though typically zeroed).
//  - **Numerical Gradient Checking (Sanity Check - Slow):** For a few operations, compare analytical gradients (from MSL kernels) with numerical gradients `(loss(param + eps) - loss(param - eps)) / (2*eps)`. This is slow but can catch fundamental errors in gradient formulas.
//  - **End-to-End Gradient Test (Small Model):** Create a tiny model (e.g., 1 layer). Perform forward, calculate loss. Perform backward. Apply a tiny change to an input. Re-run forward, get new loss. Compare `(new_loss - old_loss) / tiny_input_change` with the `grad_input` computed by the full backward pass for that input. They should be close.
// Applies to: MSL gradient kernels, Host C++ test harnesses.
