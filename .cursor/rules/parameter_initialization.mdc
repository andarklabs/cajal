---
description: Cursorrules for Parameter Initialization (.cursor/rules/parameter_initialization.mdc)
globs: 
alwaysApply: false
---
// Cursorrules for Parameter Initialization (.cursor/rules/parameter_initialization.mdc)

// Phase: 3 - Model Assembly & Training Orchestration
// Component: Task 3.2 Parameter Initialization

// Rule: Initialization Schemes
// Description: Implement standard weight initialization schemes like Xavier/Glorot (for layers with tanh/sigmoid-like activations) and Kaiming/He (for ReLU-like activations) in C++.
// These functions will generate weight values on the CPU.
// Applies to: Host C++ code.

// Rule: Filling MTLBuffers
// Description: After `MTLBuffer`s for weights and biases are allocated (by the model definition component), populate them with values generated by the initialization schemes.
// This involves getting a pointer to the buffer's contents (`buffer.contents()`), casting it to the appropriate type (e.g., `half*` or `float*`), and filling it with the CPU-generated weights.
// Ensure proper synchronization if buffers are not `MTLStorageModeShared` (though we default to shared).
// Applies to: Host C++ code.

// Rule: Data Type Consistency
// Description: Initialize weights and biases with the data type intended for the `MTLBuffer` (e.g., generate `float` values on CPU then cast to `half` if buffers are `half`, or generate `half` directly if utilities allow).
// Be mindful of precision loss if converting from `float` to `half` during initialization.
// Applies to: Host C++ code.

// Rule: Bias Initialization
// Description: Biases are often initialized to zero, but some schemes might suggest small constant values for specific layers. Follow common practices or specified model architecture details.
// Applies to: Host C++ code.

// Rule: Configurable Seed
// Description: Allow the random number generator used for weight initialization to be seeded (e.g., via `TransformerConfig`) for reproducible initial weights, which is crucial for debugging and consistent training runs.
// Applies to: Host C++ code.

// Rule: Layer-Specific Initialization (If Needed)
// Description: Some Transformer architectures might suggest slightly different initialization parameters for different layer types (e.g., smaller variance for output layers or deeper layers). The initialization logic should be flexible enough to accommodate this if required.
// Applies to: Host C++ code.

// Rule: Loading Pre-trained Weights (Future Consideration)
// Description: While the primary goal is training from scratch, design the parameter loading mechanism with an eye towards potentially loading pre-trained weights in the future. This would involve mapping named tensors from a checkpoint file to the model's `MTLBuffer`s.
// Applies to: Host C++ code (design consideration).

// Rule: Testing (TDD) - Parameter Initialization
// Description:
//  - Test individual initialization functions (Xavier, Kaiming, zero bias) on the CPU: for a given shape, verify that the generated values have the expected statistical properties (mean, variance within a range).
//  - Test filling `MTLBuffer`s: 
//    1. Create a small `MTLBuffer`.
//    2. Fill it with known values using the parameter initialization utility.
//    3. Read back the buffer contents (`memcpy` from `buffer.contents()` to a CPU array).
//    4. Verify that the read-back values match the known values (considering `float`/`half` conversions if any).
//  - Test reproducibility: Initialize parameters twice with the same seed and verify identical weights are generated and loaded.
// Applies to: Host C++ code.
