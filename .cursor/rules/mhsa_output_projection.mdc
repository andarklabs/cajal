---
description: Cursorrules for MHSA Output Projection (rules/mhsa_output_projection.mdc)
globs: 
alwaysApply: false
---
// Cursorrules for MHSA Output Projection (rules/mhsa_output_projection.mdc)

// Phase: 2 - MSL Kernel Implementation
// Component: Task 2.3.3 Multi-Head Self-Attention - Concatenation and Output Projection

// Rule: Kernel Function Signature
// Description: Kernel to concatenate head outputs (if not already contiguous) and apply the final linear projection.
// Inputs: Context vectors from attention (B, H, S, D_h), Output projection weight matrix Wo. Output: MHSA final output.
// Example: `kernel void mhsa_output_projection(device const half* context_vectors [[buffer(0)]], device const half* Wo [[buffer(1)]], device half* final_output [[buffer(2)]], constant uint batch_size [[buffer(3)]], constant uint sequence_length [[buffer(4)]], constant uint embedding_dim [[buffer(5)]], constant uint num_heads [[buffer(6)]], constant uint head_dim [[buffer(7)]], /* bias if used */ uint2 gid [[thread_position_in_grid]])`
// Applies to: MSL kernel code.

// Rule: Input Context Vectors
// Description: Buffer containing context vectors from scaled dot-product attention, typically laid out as (Batch, NumHeads, SequenceLength, HeadDim) or (B, S, H, D_h). Type `half`.
// Applies to: MSL kernel code.

// Rule: Concatenation / Reshape
// Description: If context vectors are (B, H, S, D_h), they need to be reshaped to (B, S, H, D_h) and then viewed as (B, S, H*D_h = EmbeddingDim) to be ready for the output projection. This might be a logical reshape if memory is already contiguous, or require an explicit copy/shuffle kernel if not.
// Prefer data layouts from previous step that make this a no-op or simple view.
// Applies to: MSL kernel code (or a preceding utility kernel).

// Rule: Output Projection Weight Matrix (Wo)
// Description: Weight matrix Wo for the final linear layer of MHSA, typically (num_heads * head_dim  x  embedding_dim). Stored in `MTLBuffer` (MTLStorageModeShared), type `half`.
// (num_heads * head_dim) is usually equal to embedding_dim.
// Applies to: MSL kernel code, Host C++/Swift weight initialization.

// Rule: Output Buffer (Final MHSA Output)
// Description: Stores the final output of the MHSA block, typically (Batch, SequenceLength, EmbeddingDim). Type `half`.
// This output is then fed into the Add&Norm layer.
// Applies to: MSL kernel code, Host C++/Swift buffer allocation.

// Rule: Matrix Multiplication Logic
// Description: Perform `Concatenated_Context @ Wo -> FinalOutput`.
// Concatenated_Context is (B, S, EmbDim), Wo is (EmbDim, EmbDim). Output is (B, S, EmbDim).
// Optimize using `simdgroup_matrix` or manual blocking if appropriate.
// Applies to: MSL kernel code.

// Rule: Data Types
// Description: Use `half` for inputs, weights, and outputs. Biases (if used) also `half`.
// Applies to: MSL kernel code.

// Rule: Dispatch Grid
// Description: Depends on matmul strategy. If each thread computes one output element: (batch_size * sequence_length * embedding_dim). More likely, a 2D grid (e.g., mapping to output rows and columns) with threadgroups computing tiles of the output.
// Applies to: Host C++/Swift kernel dispatch code.

// Rule: Biases
// Description: Decide whether to include a bias term in the output projection. If yes, add after matrix multiplication. Store in `MTLBuffer` (`half`).
// Applies to: MSL kernel code, Host C++/Swift weight initialization.

// Rule: Gradient Considerations (Forward-looking)
// Description: For backpropagation, gradients are needed for Wo, bias (if any), and the input concatenated context vectors.
// Applies to: Design consideration for future backward pass.

// Rule: Testing (TDD) - MHSA Output Projection Kernel
// Description:
//  - Prepare a small batch of input context vectors (e.g., 1 batch x 1 head x 2 seq_len x 2 head_dim, then reshaped/concatenated to 1 batch x 2 seq_len x 2 total_dim if head_dim*num_heads = total_dim).
//  - Initialize a small, known output weight matrix Wo (e.g., 2x2 if total_dim = embedding_dim = 2) and bias (if used).
//  - Manually (or with Python/NumPy) calculate the expected final output of the MHSA block.
//  - Execute the MSL mhsa_output_projection kernel.
//  - Read back the final output buffer from GPU and compare with the expected result (element-wise with tolerance).
//  - Test correct handling of reshaping/concatenation of head outputs before projection.
// Applies to: MSL kernel code, Host C++/Swift test harness.
