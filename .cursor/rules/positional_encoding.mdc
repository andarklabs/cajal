---
description: Cursorrules for Positional Encoding (rules/positional_encoding.mdc)
globs: 
alwaysApply: false
---
// Cursorrules for Positional Encoding (rules/positional_encoding.mdc)

// Phase: 2 - MSL Kernel Implementation
// Component: Task 2.2 Positional Encoding

// Rule: Method - Sinusoidal Precomputed
// Description: Default to using precomputed sinusoidal positional encodings. These should be calculated on the CPU (once) and stored in an `MTLBuffer` with `MTLStorageModeShared`.
// Applies to: Host C++/Swift precomputation, MSL kernel for applying encodings.

// Rule: Kernel Function Signature (for applying precomputed PE)
// Description: If applying precomputed PEs, the kernel will take the input embeddings (output from embedding layer), the precomputed positional encoding table, and the output buffer (embeddings + PE).
// Example: `kernel void apply_positional_encoding(device half* input_embeddings [[buffer(0)]], device const half* pe_table [[buffer(1)]], device half* output_embeddings [[buffer(2)]], constant uint sequence_length [[buffer(3)]], constant uint embedding_dim [[buffer(4)]], uint2 gid [[thread_position_in_grid]])` (gid.x for sequence, gid.y for batch index)
// Applies to: MSL kernel code.

// Rule: Input/Output Embeddings
// Description: Input embeddings (batch_size x sequence_length x embedding_dim) are modified in-place or written to an output buffer after adding positional encodings. Data type is typically `half` or `float`.
// Applies to: MSL kernel code.

// Rule: Positional Encoding Table
// Description: The PE table buffer (max_sequence_length x embedding_dim) contains the precomputed sinusoidal values. The kernel reads the appropriate row based on the token's position in the sequence.
// Applies to: MSL kernel code, Host C++/Swift precomputation and buffer creation.

// Rule: Application Logic
// Description: The kernel adds the positional encoding vector (for a given position) to the token embedding vector element-wise.
// `output_embedding[batch_idx][seq_idx][dim_idx] = input_embedding[batch_idx][seq_idx][dim_idx] + pe_table[seq_idx][dim_idx]`
// Applies to: MSL kernel code.

// Rule: Dispatch Grid
// Description: The host code should dispatch a 2D grid (batch_size, sequence_length * embedding_dim elements if unrolling inner loop, or batch_size * sequence_length if each thread handles a full vector). A common approach is for each thread to handle one token's full embedding vector: grid (batch_size, sequence_length).
// Applies to: Host C++/Swift kernel dispatch code.

// Rule: Data Types
// Description: Use `half` or `float` for positional encodings, consistent with the embedding vectors.
// Applies to: MSL kernel code, Host C++/Swift precomputation.

// Rule: Learned Positional Embeddings (Alternative)
// Description: If learned positional embeddings are chosen (see plan.md), this component would be similar to another embedding layer (Task 2.1), where position indices (0 to seq_len-1) are used to look up learned PE vectors. This requires its own weight matrix and gradient handling.
// Applies to: Alternative MSL kernel design.

// Rule: On-the-fly Calculation (Alternative)
// Description: Sinusoidal PEs can be calculated directly within the kernel. This avoids storing a PE table but adds computation to the kernel. Precomputing is generally preferred unless memory for the PE table is a major constraint or dynamic sequence lengths beyond a precomputed max are common.
// Applies to: Alternative MSL kernel design.

// Rule: Maximum Sequence Length
// Description: The precomputed PE table should be sized for the maximum expected sequence length during training and inference. If longer sequences are encountered, this needs to be handled (e.g., error, or a strategy like PE interpolation, though this adds complexity).
// Applies to: Host C++/Swift precomputation, Model configuration.

// Rule: Testing (TDD) - Positional Encoding Kernel
// Description:
//  - Test CPU-side sinusoidal PE table generation: for a given max_seq_len and embedding_dim, verify a few calculated PE vectors against a reference formula.
//  - For the MSL kernel (applying PE):
//    - Prepare a small batch of input embedding vectors (e.g., 2 batch x 3 seq_len x 4 dim) initialized to known values (e.g., all zeros or all ones).
//    - Prepare a small PE table (e.g., 3 seq_len x 4 dim) with known values.
//    - Manually (or with Python/NumPy) calculate the expected output embeddings (input + PE).
//    - Execute the MSL apply_positional_encoding kernel.
//    - Read back and compare GPU output with expected output (element-wise with tolerance).
//  - If learned PEs are used, test its lookup like the token embedding layer.
// Applies to: MSL kernel code, Host C++/Swift PE generation and test harness.
