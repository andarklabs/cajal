---
description: Cursorrules for Training Loop (.cursor/rules/training_loop.mdc)
globs: 
alwaysApply: false
---
// Cursorrules for Training Loop (.cursor/rules/training_loop.mdc)

// Phase: 3 - Model Assembly & Training Orchestration
// Component: Task 3.3 Training Loop

// Rule: Main Loop Structure
// Description: Implement the main training loop in C++. This loop iterates over epochs and, within each epoch, iterates over batches of data from the training set.
// Responsibilities: data batching, forward pass, loss calculation, backward pass, optimizer step, metrics logging.
// Applies to: Host C++ code.

// Rule: Data Batching
// Description: Interface with the data loading/preprocessing component to get batches of tokenized and padded input sequences (`input_ids`) and target sequences (`target_ids`).
// These batches should be readily convertible to `MTLBuffer`s.
// Applies to: Host C++ code.

// Rule: MTLCommandBuffer Management
// Description: For each training step (batch), create a new `MTLCommandBuffer`. All MSL kernels for that step (forward, loss, backward, optimizer) will be encoded into this command buffer.
// Commit the command buffer and handle its completion (e.g., `waitUntilCompleted` for simplicity initially, or asynchronous completion handlers for advanced pipelining).
// Applies to: Host C++ code.

// Rule: Forward Pass Invocation
// Description: Call the model's `forward` method, passing the input batch `MTLBuffer` and the current `MTLCommandBuffer`. This populates the command buffer with forward pass kernel dispatches. The final output should be logits in an `MTLBuffer`.
// Applies to: Host C++ code.

// Rule: Loss Calculation Invocation
// Description: After the forward pass, invoke the loss calculation kernel (e.g., CrossEntropyLoss). This kernel takes logits and target_ids `MTLBuffer`s as input and outputs a scalar loss value into an `MTLBuffer`.
// The loss value needs to be read back to the CPU for logging/monitoring.
// Applies to: Host C++ code.

// Rule: Backward Pass Invocation
// Description: Invoke the backward pass orchestration method. This will encode all gradient calculation kernels, starting from the gradient of the loss w.r.t. logits, and backpropagating through the model layers.
// Requires access to stored activations from the forward pass.
// Applies to: Host C++ code.

// Rule: Optimizer Step Invocation
// Description: After gradients for all parameters are computed and stored in their respective gradient `MTLBuffer`s, invoke the optimizer's `step` method. This will encode kernels to update model weights using the computed gradients.
// Applies to: Host C++ code.

// Rule: Gradient Zeroing
// Description: Before each backward pass (or at the start of each training step), ensure all gradient buffers are zeroed out. This can be done with a `MTLBlitCommandEncoder` fillBuffer operation or a custom MSL kernel.
// Applies to: Host C++ code.

// Rule: Metrics Logging
// Description: Log relevant training metrics: loss (per batch/epoch), learning rate, throughput (tokens/sec), perplexity (optional). Output to console and potentially a log file or metrics dashboard (e.g., TensorBoard-like format if integrating external tools).
// Applies to: Host C++ code.

// Rule: Checkpointing
// Description: Implement model checkpointing: periodically save the model's state (weights, optimizer state, current epoch/batch) to disk. This allows resuming training and saving the best model.
// Weights should be read back from `MTLBuffer`s to CPU before saving.
// Applies to: Host C++ code.

// Rule: Evaluation/Validation Loop
// Description: Periodically (e.g., after each epoch), run an evaluation loop on a validation dataset. This involves a forward pass and loss calculation (no backward pass or optimizer step). Report validation metrics.
// Applies to: Host C++ code.

// Rule: Error Handling in Loop
// Description: Implement robust error handling within the training loop for Metal API errors, data loading issues, etc. Allow for graceful shutdown or recovery if possible.
// Applies to: Host C++ code.

// Rule: Testing (TDD) - Training Loop Orchestration
// Description:
//  - Test batch iteration: ensure the loop correctly iterates through a mock dataset.
//  - Test `MTLCommandBuffer` creation and commitment for a single step.
//  - Mock model forward/backward/optimizer steps: initially, these can be simple placeholder functions in C++ that just log their invocation.
//  - Test the sequence of calls: forward -> loss -> backward -> optimizer_step -> gradient_zeroing is correct for a training step.
//  - Test loss retrieval: mock a loss buffer and ensure its value can be read back.
//  - Test metrics logging: verify that mock loss values are correctly logged.
//  - Test checkpointing logic: mock model parameters, trigger a save, verify that data is written (e.g., to a dummy file with expected content structure).
//  - Test evaluation loop logic: ensure it calls forward/loss but not backward/optimizer.
//  - As actual MSL kernels and model components become available, replace mocks with real calls in integration tests for the training loop.
// Applies to: Host C++ code.
